{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_NLP_With_Torch_2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPOFuZXxm/eCDUNIZs5lFnW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjeevr5/NLP_Excercises/blob/main/DL_NLP_With_Torch_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-class classification problem using RNNs\n",
        "\n",
        "This exercise is performed on AGNews dataset with four news categories:\n",
        "\n",
        "1 : World\n",
        "2 : Sports\n",
        "3 : Business\n",
        "4 : Sci/Tec \n",
        "\n",
        "<b> Architecture details </b>\n",
        "\n",
        "1. We shall use bpeMB word representations as word vectors. This is a sub word tokenization process with a vector for every sub token. Sub-word tokens can make the sentences longer and hence to be used with care.\n",
        "2. We will just use the titles to predict the news category.\n",
        "3. This RNN architecture does not pad sequences, we shall feed the packed sequences directly to the embedding layer.\n",
        "4. Dataloaders to have generator kind of data feed to the model.\n",
        "5. Passing outputs of output layer or the hidden state to the dense layer."
      ],
      "metadata": {
        "id": "WcXdNoqpCO-k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the data"
      ],
      "metadata": {
        "id": "_vy2VG0zCij6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4fSbz_pCI9P"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install bpemb\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the essentials"
      ],
      "metadata": {
        "id": "7j8a47yRCkQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils import data\n",
        "from torch.nn.utils.rnn import pack_sequence\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from bpemb import BPEmb\n",
        "\n",
        "\n",
        "bpemb_en = BPEmb(lang=\"en\", dim=300, vs = 10000) #vs will be the voacb size\n",
        "print(bpemb_en.vectors.shape)\n",
        "embed_matrix = torch.tensor(bpemb_en.vectors)\n",
        "SEED = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "v9_P9uMgCbYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "156cea3e-a916-40b9-9f8b-b2b8da278f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs25000.model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 661443/661443 [00:00<00:00, 15653630.99B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs25000.d300.w2v.bin.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28009055/28009055 [00:00<00:00, 56909932.83B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer and pre-processing\n",
        "\n",
        "A minimal pre-processing to retain only characters."
      ],
      "metadata": {
        "id": "xrKPOdUDCmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('./train.csv', sep = ',', header = None)\n",
        "test_data = pd.read_csv('./test.csv', sep = ',', header = None)\n",
        "\n",
        "print(f'Train shape : {train_data.shape} test shape : {test_data.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EWM0XDde_h_",
        "outputId": "1e5871f8-21dc-4b7f-c537-4985e425828a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape : (120000, 3) test shape : (7600, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5L2WcF6QfdqL",
        "outputId": "6815b59c-a995-4336-9b48-c6da78bc4a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0                                                  1  \\\n",
              "0  3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
              "1  3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
              "2  3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
              "3  3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
              "4  3  Oil prices soar to all-time record, posing new...   \n",
              "\n",
              "                                                   2  \n",
              "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
              "1  Reuters - Private investment firm Carlyle Grou...  \n",
              "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
              "3  Reuters - Authorities have halted oil export\\f...  \n",
              "4  AFP - Tearaway world oil prices, toppling reco...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f24995b2-12b9-44ee-9b23-e96de8dd5eb2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f24995b2-12b9-44ee-9b23-e96de8dd5eb2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f24995b2-12b9-44ee-9b23-e96de8dd5eb2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f24995b2-12b9-44ee-9b23-e96de8dd5eb2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = train_data.iloc[:,1].map(lambda x : x.split(' (')[0].lower()), train_data.iloc[:,0].map(lambda x : int(x - 1))\n",
        "X_test, y_test = test_data.iloc[:,1].map(lambda x : x.split(' (')[0].lower()), test_data.iloc[:,0].map(lambda x : int(x - 1))"
      ],
      "metadata": {
        "id": "hAclfea9fjjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encoded = [torch.tensor(bpemb_en.encode_ids(item)) for item in X_train.values]\n",
        "test_encoded = [torch.tensor(bpemb_en.encode_ids(item)) for item in X_test.values]\n",
        "\n",
        "print('train sample:', train_encoded[0])\n",
        "print('test sample:', test_encoded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siDIR_pugNVe",
        "outputId": "663cb9e7-c369-41c7-eab0-494d1c9dce6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train sample: tensor([ 2029,    66, 24935,  7820, 24697,   810,   423,     7,  1149])\n",
            "test sample: tensor([15923,    72,     3,    47, 14396,   297,  9981])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader with padding"
      ],
      "metadata": {
        "id": "rtUaddNODTFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class Data_Iterator(data.Dataset):\n",
        "\n",
        "  def __init__(self, text, label):\n",
        "    super(Data_Iterator, self).__init__()\n",
        "    assert len(text) == len(label)\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.text[index], self.label[index]\n",
        "\n",
        "train_data = Data_Iterator(train_encoded, y_train)\n",
        "test_data = Data_Iterator(test_encoded, y_test)"
      ],
      "metadata": {
        "id": "3sPb4POXsn2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence, pad_packed_sequence\n",
        "\n",
        "def packed(batch):\n",
        "  sorted_batch = sorted(batch, key=lambda x: x[0].shape[0], reverse=True)\n",
        "  text_seq = [i[0] for i in sorted_batch]\n",
        "  text_seq = pack_sequence(text_seq, enforce_sorted=False).to(device)\n",
        "  target = torch.LongTensor([i[1] for i in sorted_batch]).to(device)\n",
        "  return text_seq, target\n",
        "\n",
        "trainloader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn= packed) \n",
        "testloader = DataLoader(test_data, batch_size=128, shuffle=True, collate_fn = packed)"
      ],
      "metadata": {
        "id": "-Iw9hN9UDVdn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq_Model(nn.Module):\n",
        "\n",
        "  def __init__(self, embedding_size, lstm_hidden_units, n_layers, n_classes, bi_d = True, drop_rate = 0.3):\n",
        "    super(Seq_Model, self).__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding.from_pretrained(torch.as_tensor(embed_matrix))\n",
        "    self.seq = nn.LSTM(embedding_size, lstm_hidden_units, num_layers = n_layers, bidirectional = bi_d, dropout = drop_rate)\n",
        "    self.fc = nn.Linear(2 * lstm_hidden_units if bi_d else lstm_hidden_units, n_classes)\n",
        "    self.dropout = nn.Dropout(drop_rate)\n",
        "    self.bi_d = bi_d\n",
        "\n",
        "  def forward(self, input_batch):\n",
        "    \"\"\"\n",
        "      ref : \"https://discuss.pytorch.org/t/how-to-use-pack-sequence-if-we-are-going-to-use-word-embedding-and-bilstm/28184/4\"\n",
        "      the problem is packed sequences can be directly fed to LSTM/RNN but not to the embedding layer!\n",
        "    \"\"\"\n",
        "    #input_batch = [sent len, batch size]\n",
        "\n",
        "    #embed = self.dropout(simple_elementwise_apply(self.embedding, input_batch))#self.embedding(simple_elementwise_applyinput_batch))\n",
        "    #embed = [seq_len, batch_size, embed_dim]\n",
        "    embed = simple_elementwise_apply(self.embedding, input_batch)\n",
        "    packed_output, (hidden, cell) = self.seq(embed)\n",
        "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1) if self.bi_d else hidden[-1])\n",
        "    return self.fc(hidden)\n",
        "\n",
        "\n",
        "def simple_elementwise_apply(fn, packed_sequence):\n",
        "    \"\"\"applies a pointwise function fn to each element in packed_sequence\"\"\"\n",
        "    return torch.nn.utils.rnn.PackedSequence(fn(packed_sequence.data), packed_sequence.batch_sizes)\n",
        "\n",
        "model = Seq_Model(300, 512, 3, 4)\n",
        "print(f'The number of trainable parameters are : {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk2lzcX2tG35",
        "outputId": "87cd7809-267a-4981-af35-f469958c8077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of trainable parameters are : 15,937,540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "metadata": {
        "id": "TZh1bgauDjMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def accuracy(preds, true):\n",
        "  _, index = torch.max(preds, dim = 1)\n",
        "  return (index == true).sum().float() / len(preds)"
      ],
      "metadata": {
        "id": "rsrrNUC-GLHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_m(model, iterator, optimizer, l):\n",
        "  e_loss = 0\n",
        "  e_acc = 0\n",
        "  model.train()\n",
        "\n",
        "  for inputs, labels in iterator:\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(inputs)\n",
        "    acc = accuracy(preds,  labels)\n",
        "    loss = l(preds.squeeze(1), labels.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    e_loss += loss.item()\n",
        "    e_acc += acc.item()\n",
        "  return e_loss/len(iterator), e_acc/len(iterator)\n",
        "\n",
        "def evaluate_m(model, iterator, l):\n",
        "  e_loss = 0\n",
        "  e_acc = 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for inputs, labels in iterator:\n",
        "      preds = model(inputs)\n",
        "      loss = l(preds.squeeze(1), labels.long())\n",
        "      acc = accuracy(preds,  labels)\n",
        "      e_loss += loss.item()\n",
        "      e_acc += acc.item()\n",
        "  return e_loss/len(iterator), e_acc/len(iterator)"
      ],
      "metadata": {
        "id": "DDi_HVv9G7Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train_m(model, trainloader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate_m(model, testloader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s8TtxnFHX7e",
        "outputId": "69461185-1e13-44e7-9b58-7c49bad24696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 / 20 | Epoch Time: 1m 10s\n",
            "\tTrain Loss: 1.216 | Train Acc: 48.54%\n",
            "\t Val. Loss: 1.382 |  Val. Acc: 35.53%\n",
            "Epoch: 02 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.155 | Train Acc: 50.79%\n",
            "\t Val. Loss: 1.379 |  Val. Acc: 36.04%\n",
            "Epoch: 03 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.134 | Train Acc: 51.66%\n",
            "\t Val. Loss: 1.400 |  Val. Acc: 36.61%\n",
            "Epoch: 04 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.119 | Train Acc: 52.69%\n",
            "\t Val. Loss: 1.390 |  Val. Acc: 35.11%\n",
            "Epoch: 05 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.109 | Train Acc: 53.20%\n",
            "\t Val. Loss: 1.408 |  Val. Acc: 35.79%\n",
            "Epoch: 06 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.091 | Train Acc: 54.16%\n",
            "\t Val. Loss: 1.421 |  Val. Acc: 36.68%\n",
            "Epoch: 07 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.084 | Train Acc: 54.46%\n",
            "\t Val. Loss: 1.415 |  Val. Acc: 34.89%\n",
            "Epoch: 08 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.079 | Train Acc: 54.79%\n",
            "\t Val. Loss: 1.406 |  Val. Acc: 35.84%\n",
            "Epoch: 09 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.068 | Train Acc: 55.43%\n",
            "\t Val. Loss: 1.439 |  Val. Acc: 35.65%\n",
            "Epoch: 10 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.064 | Train Acc: 55.50%\n",
            "\t Val. Loss: 1.472 |  Val. Acc: 35.37%\n",
            "Epoch: 11 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.064 | Train Acc: 55.51%\n",
            "\t Val. Loss: 1.410 |  Val. Acc: 36.94%\n",
            "Epoch: 12 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.060 | Train Acc: 55.79%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 35.57%\n",
            "Epoch: 13 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.058 | Train Acc: 55.83%\n",
            "\t Val. Loss: 1.446 |  Val. Acc: 35.03%\n",
            "Epoch: 14 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.055 | Train Acc: 56.00%\n",
            "\t Val. Loss: 1.437 |  Val. Acc: 35.62%\n",
            "Epoch: 15 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.052 | Train Acc: 56.11%\n",
            "\t Val. Loss: 1.438 |  Val. Acc: 35.50%\n",
            "Epoch: 16 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.050 | Train Acc: 56.43%\n",
            "\t Val. Loss: 1.455 |  Val. Acc: 35.57%\n",
            "Epoch: 17 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.046 | Train Acc: 56.46%\n",
            "\t Val. Loss: 1.484 |  Val. Acc: 34.50%\n",
            "Epoch: 18 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.051 | Train Acc: 56.18%\n",
            "\t Val. Loss: 1.464 |  Val. Acc: 36.21%\n",
            "Epoch: 19 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.048 | Train Acc: 56.42%\n",
            "\t Val. Loss: 1.459 |  Val. Acc: 36.15%\n",
            "Epoch: 20 / 20 | Epoch Time: 1m 9s\n",
            "\tTrain Loss: 1.044 | Train Acc: 56.81%\n",
            "\t Val. Loss: 1.472 |  Val. Acc: 36.61%\n"
          ]
        }
      ]
    }
  ]
}