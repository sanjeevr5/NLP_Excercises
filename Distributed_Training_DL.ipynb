{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43298b74-0ebc-42f4-88bb-409f4a3c34d9",
   "metadata": {},
   "source": [
    "# Distributed PyTorch training With Azure ML\n",
    "\n",
    "<b> Objective : To create a multiclass classification model with PyTorch on Azure ML</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b67e4d-3707-49b2-8662-73968e4a7faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!mkdir ./newsDataset\n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv -P ./newsDataset\n",
    "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv -P ./newsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "504f30b3-007b-428f-8b5d-9e9d357fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "dstore.upload('./newsDataset', '/news-dataset', show_progress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062050a-8352-4501-81bf-563f9ffe40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Registering the dataset\n",
    "from azureml.core import Dataset\n",
    "\n",
    "trainDataset =  Dataset.Tabular.from_delimited_files(dstore.path('news-dataset/train.csv'))\n",
    "testDataset =  Dataset.Tabular.from_delimited_files(dstore.path('news-dataset/test.csv'))\n",
    "\n",
    "trainDataset.register(workspace=ws, name='news-train-dataset', description='news train dataset')\n",
    "testDataset.register(workspace=ws, name='news-test-dataset', description='news test dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77e74f91-1fa6-42b8-bdd9-a32940c7efb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2022-09-14T13:11:08.250000+00:00', 'errors': None, 'creationTime': '2022-09-14T11:05:19.257863+00:00', 'modifiedTime': '2022-09-14T11:05:22.841283+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 2, 'nodeIdleTimeBeforeScaleDown': 'PT1800S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_D11_V2'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = 'pytorch-cluster'\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_D11_v2',\n",
    "                                                           max_nodes=2) #Not enough quota on trial account\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8eba181-c6ed-4de8-ab6a-d681ee7c4629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property environment_variables is deprecated. Use RunConfiguration.environment_variables to set runtime variables.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"assetId\": \"azureml://locations/centralindia/workspaces/a48b0d16-eea3-472f-8972-07820f686339/environments/Pytorch-train/versions/6\",\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:20220708.v1\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"buildContext\": null,\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"clientId\": \"3f437a83-9aaa-4632-935f-e703b210ca06\",\n",
       "        \"clientSecret\": \"yQp8Q~LatQDoUSnofy3B216YDrMXlO3N6TvXrceK\",\n",
       "        \"tenantId\": \"bfaf20f3-e8b6-4051-b061-f60e6aaf6bce\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"Pytorch-train\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.8.13\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"torchtext\",\n",
       "                        \"more-itertools\",\n",
       "                        \"pandas\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"project_environment\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"6\"\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import json\n",
    "\n",
    "with open('./modelTrain/SP.json') as f:\n",
    "    sp = json.load(f)\n",
    "\n",
    "env = Environment(name = 'Pytorch-train')\n",
    "conda_dep = CondaDependencies()\n",
    "\n",
    "packages = ['torchtext', 'more-itertools', 'pandas']\n",
    "for package in packages:\n",
    "    conda_dep.add_pip_package(package)\n",
    "\n",
    "env.environment_variables = {'clientId': sp['clientId'], 'clientSecret' : sp['clientSecret'], 'tenantId' : sp['tenantId']}\n",
    "    \n",
    "# Adds dependencies to PythonSection of myenv\n",
    "env.python.conda_dependencies = conda_dep\n",
    "\n",
    "env.register(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a84162a5-a67d-468d-a350-b9191f098ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, ScriptRunConfig, Environment, Experiment\n",
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "curated_env_name = 'Pytorch-train'\n",
    "pytorch_env = Environment.get(workspace=ws, name=curated_env_name)\n",
    "distr_config = MpiConfiguration(process_count_per_node = 2, node_count = 2)\n",
    "\n",
    "args = ['--num_classes', 4, '--embed_size', 128, '--max_width', 35]\n",
    "\n",
    "run_config = ScriptRunConfig(\n",
    "  source_directory= './modelTrain',\n",
    "  arguments = args,\n",
    "  script='Pytorch_train.py',\n",
    "  compute_target=compute_target,\n",
    "  environment=pytorch_env,\n",
    "  distributed_job_config=distr_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3711bb61-c337-4f73-afd9-108ffeb5505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = Experiment(ws, \"torch-exp2\").submit(run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6403d567-241a-4d20-985f-7558f8ec75c2",
   "metadata": {},
   "source": [
    "# PyTorch Script - Model with training and evaluation script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7974b92-50ed-451f-8796-851b06c402db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting more-itertools\n",
      "  Downloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 376 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: more-itertools\n",
      "Successfully installed more-itertools-8.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install more-itertools\n",
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc93285-3863-40f1-8713-d8a1a7b5fa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import more_itertools as mit\n",
    "import time\n",
    "from azureml.core import Workspace, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "sp = ServicePrincipalAuthentication(tenant_id= os.environ['tenantId'], # tenantID\n",
    "                                    service_principal_id= os.environ['clientId'], # clientId\n",
    "                                    service_principal_password = os.environ['clientSecret']) # clientSecret\n",
    "\n",
    "global MAX_WIDTH\n",
    "\n",
    "SEED= 20\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ws = Workspace.get(name= 'mlops-san',\n",
    "                   auth=sp,\n",
    "                   subscription_id= '0d1442c1-d386-4505-9abe-0bedfd63701e',\n",
    "                   resource_group= 'mlops-san')\n",
    "\n",
    "train_dataset = Dataset.get_by_name(ws, name='news-train-dataset')\n",
    "train_df = train_dataset.to_pandas_dataframe()\n",
    "\n",
    "test_dataset = Dataset.get_by_name(ws, name='news-test-dataset')\n",
    "test_df = test_dataset.to_pandas_dataframe()\n",
    "\n",
    "train_text, train_label = train_df.iloc[:, 2].map(lambda x : x.lower()), train_df.iloc[:, 0].map(lambda x : int(x) - 1)\n",
    "test_text, test_label = test_df.iloc[:, 2].map(lambda x : x.lower()), test_df.iloc[:, 0].map(lambda x : int(x) - 1)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def vocabGenerator(lst):\n",
    "      for data in lst:\n",
    "        yield list(ngrams_iterator(tokenizer(data), 2))\n",
    "\n",
    "vocab = build_vocab_from_iterator(vocabGenerator(train_text.values.tolist()), min_freq = 5, specials = ['<unk>', '<pad>'], max_tokens = 30002, special_first = True)\n",
    "vocab.set_default_index(vocab['<unk>']) #unknown tokens will have this index\n",
    "\n",
    "def collator(batch):\n",
    "    text = [list(mit.padded(vocab(tokenizer(i[0])), vocab['<pad>'], MAX_WIDTH))[:MAX_WIDTH] for i in batch]\n",
    "    label = [i[1] for i in batch]\n",
    "    return torch.tensor(text), torch.LongTensor(label)\n",
    "\n",
    "def dataBatcher(batch, bs = 32):\n",
    "    return DataLoader(batch, batch_size = bs, shuffle = True, collate_fn = collator)\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dims, pad_index, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dims, pad_index)\n",
    "        self.fc = nn.Linear(embed_dims, n_classes) \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs) #batch_size, seq_len, embed_size\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) #returns batch_size, embed_size\n",
    "        return self.fc(pooled)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def accuracy(preds, true):\n",
    "    _, index = torch.max(preds, dim = 1)\n",
    "    return (index == true).sum().float() / len(preds)\n",
    "\n",
    "def train_m(model, iterator, optimizer, l):\n",
    "    e_loss = 0\n",
    "    e_acc = 0\n",
    "    model.train()\n",
    "    for inputs, labels in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        acc = accuracy(preds,  labels)\n",
    "        loss = l(preds.squeeze(1), labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        e_loss += loss.item()\n",
    "        e_acc += acc.item()\n",
    "    return e_loss/len(iterator), e_acc/len(iterator)\n",
    "\n",
    "def evaluate_m(model, iterator, l):\n",
    "    e_loss = 0\n",
    "    e_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in iterator:\n",
    "            preds = model(inputs)\n",
    "            loss = l(preds.squeeze(1), labels.long())\n",
    "            acc = accuracy(preds,  labels)\n",
    "            e_loss += loss.item()\n",
    "            e_acc += acc.item()\n",
    "    return e_loss/len(iterator), e_acc/len(iterator)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_epochs', type=int, default=2, help='Number of epochs to train')\n",
    "    parser.add_argument('--exp_name', type=str, help='Name of MLFlow experiment')\n",
    "    parser.add_argument('--max_width', type=int, help='Maximum width including padding')\n",
    "    parser.add_argument('--num_classes', type=int, help='Number of classes')\n",
    "    parser.add_argument('--embed_size', type=int, default = 128, help='Embed vector size')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #call from main script\n",
    "    global MAX_WIDTH\n",
    "    MAX_WIDTH = args.max_width\n",
    "    trainDataloader = dataBatcher([[i, j] for i,j in zip(train_text.values.tolist(), train_label.values.tolist())])\n",
    "    testDataloader = dataBatcher([[i, j] for i,j in zip(test_text.values.tolist(), test_label.values.tolist())])\n",
    "    #end of call from main script\n",
    "    \n",
    "    model = FastText(len(vocab), args.embed_size, vocab['<pad>'], args.num_classes)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = args.num_epochs\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_m(model, trainDataloader, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate_m(model, testDataloader, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0272d1-46f8-4028-96a8-fa206a263615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dims, pad_index, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dims, pad_index)\n",
    "        self.fc = nn.Linear(embed_dims, n_classes) \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs) #batch_size, seq_len, embed_size\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) #returns batch_size, embed_size\n",
    "        return self.fc(pooled)\n",
    "\n",
    "model = FastText(len(vocab), 128, vocab['<pad>'], 4)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0664c4-ea73-441b-a4b8-80a14143fcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def accuracy(preds, true):\n",
    "    _, index = torch.max(preds, dim = 1)\n",
    "    return (index == true).sum().float() / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab08609-25eb-4518-8b4d-6365bef961d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_m(model, iterator, optimizer, l):\n",
    "    e_loss = 0\n",
    "    e_acc = 0\n",
    "    model.train()\n",
    "    for inputs, labels in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        acc = accuracy(preds,  labels)\n",
    "        loss = l(preds.squeeze(1), labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        e_loss += loss.item()\n",
    "        e_acc += acc.item()\n",
    "    return e_loss/len(iterator), e_acc/len(iterator)\n",
    "\n",
    "def evaluate_m(model, iterator, l):\n",
    "    e_loss = 0\n",
    "    e_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in iterator:\n",
    "            preds = model(inputs)\n",
    "            loss = l(preds.squeeze(1), labels.long())\n",
    "            acc = accuracy(preds,  labels)\n",
    "            e_loss += loss.item()\n",
    "            e_acc += acc.item()\n",
    "    return e_loss/len(iterator), e_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f9d71c3-6344-4a83-86d7-13abd50a5fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 / 1 | Epoch Time: 0m 45s\n",
      "\tTrain Loss: 0.518 | Train Acc: 82.58%\n",
      "\t Val. Loss: 0.336 |  Val. Acc: 88.34%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_m(model, trainDataloader, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate_m(model, testDataloader, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "882df615-e1ed-4c81-8b14-433c7bb0b4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Pytorch_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Pytorch_train.py\n",
    "import os\n",
    "import argparse\n",
    "import more_itertools as mit\n",
    "import time\n",
    "from azureml.core import Workspace, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "sp = ServicePrincipalAuthentication(tenant_id= os.environ['tenantId'], # tenantID\n",
    "                                    service_principal_id= os.environ['clientId'], # clientId\n",
    "                                    service_principal_password = os.environ['clientSecret']) # clientSecret\n",
    "\n",
    "\n",
    "SEED= 20\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ws = Workspace.get(name= 'mlops-san',\n",
    "                   auth=sp,\n",
    "                   subscription_id= '0d1442c1-d386-4505-9abe-0bedfd63701e',\n",
    "                   resource_group= 'mlops-san')\n",
    "\n",
    "train_dataset = Dataset.get_by_name(ws, name='news-train-dataset')\n",
    "train_df = train_dataset.to_pandas_dataframe()\n",
    "\n",
    "test_dataset = Dataset.get_by_name(ws, name='news-test-dataset')\n",
    "test_df = test_dataset.to_pandas_dataframe()\n",
    "\n",
    "train_text, train_label = train_df.iloc[:, 2].map(lambda x : x.lower()), train_df.iloc[:, 0].map(lambda x : int(x) - 1)\n",
    "test_text, test_label = test_df.iloc[:, 2].map(lambda x : x.lower()), test_df.iloc[:, 0].map(lambda x : int(x) - 1)\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def vocabGenerator(lst):\n",
    "      for data in lst:\n",
    "        yield list(ngrams_iterator(tokenizer(data), 2))\n",
    "\n",
    "vocab = build_vocab_from_iterator(vocabGenerator(train_text.values.tolist()), min_freq = 5, specials = ['<unk>', '<pad>'], max_tokens = 30002, special_first = True)\n",
    "vocab.set_default_index(vocab['<unk>']) #unknown tokens will have this index\n",
    "\n",
    "def collator(batch):\n",
    "    text = [list(mit.padded(vocab(tokenizer(i[0])), vocab['<pad>'], MAX_WIDTH))[:MAX_WIDTH] for i in batch]\n",
    "    label = [i[1] for i in batch]\n",
    "    return torch.tensor(text), torch.LongTensor(label)\n",
    "\n",
    "def dataBatcher(batch, bs = 32):\n",
    "    return DataLoader(batch, batch_size = bs, shuffle = True, collate_fn = collator)\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dims, pad_index, n_classes):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dims, pad_index)\n",
    "        self.fc = nn.Linear(embed_dims, n_classes) \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embed(inputs) #batch_size, seq_len, embed_size\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) #returns batch_size, embed_size\n",
    "        return self.fc(pooled)\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "def accuracy(preds, true):\n",
    "    _, index = torch.max(preds, dim = 1)\n",
    "    return (index == true).sum().float() / len(preds)\n",
    "\n",
    "def train_m(model, iterator, optimizer, l):\n",
    "    e_loss = 0\n",
    "    e_acc = 0\n",
    "    model.train()\n",
    "    for inputs, labels in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        acc = accuracy(preds,  labels)\n",
    "        loss = l(preds.squeeze(1), labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        e_loss += loss.item()\n",
    "        e_acc += acc.item()\n",
    "    return e_loss/len(iterator), e_acc/len(iterator)\n",
    "\n",
    "def evaluate_m(model, iterator, l):\n",
    "    e_loss = 0\n",
    "    e_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in iterator:\n",
    "            preds = model(inputs)\n",
    "            loss = l(preds.squeeze(1), labels.long())\n",
    "            acc = accuracy(preds,  labels)\n",
    "            e_loss += loss.item()\n",
    "            e_acc += acc.item()\n",
    "    return e_loss/len(iterator), e_acc/len(iterator)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--num_epochs', type=int, default=2, help='Number of epochs to train')\n",
    "    parser.add_argument('--exp_name', type=str, help='Name of MLFlow experiment')\n",
    "    parser.add_argument('--max_width', type=str, help='Maximum width including padding')\n",
    "    parser.add_argument('--num_classes', type=int, help='Number of classes')\n",
    "    parser.add_argument('--embed_size', type=int, default = 128, help='Embed vector size')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    #call from main script\n",
    "    MAX_WIDTH = args.max_width\n",
    "    trainDataloader = dataBatcher([[i, j] for i,j in zip(train_text.values.tolist(), train_label.values.tolist())])\n",
    "    testDataloader = dataBatcher([[i, j] for i,j in zip(test_text.values.tolist(), test_label.values.tolist())])\n",
    "    #end of call from main script\n",
    "    \n",
    "    model = FastText(len(vocab), args.embed_size, vocab['<pad>'], args.num_classes)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = args.num_epochs\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train_m(model, trainDataloader, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate_m(model, testDataloader, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} / {N_EPOCHS} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
